{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H-JEPA Interactive Model Explorer\n",
    "\n",
    "This notebook provides an interactive environment for exploring trained H-JEPA models.\n",
    "\n",
    "**Features:**\n",
    "- Load and inspect model checkpoints\n",
    "- Visualize hierarchical representations\n",
    "- Analyze attention patterns\n",
    "- Explore feature activations\n",
    "- Run quick evaluations\n",
    "\n",
    "**Usage:**\n",
    "1. Set the checkpoint path below\n",
    "2. Run cells sequentially\n",
    "3. Experiment with different samples and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.models.hjepa import create_hjepa\n",
    "from src.data.datasets import get_dataset\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CONFIGURE THESE ============\n",
    "CHECKPOINT_PATH = \"../results/validation_test/checkpoints/checkpoint_epoch_15.pt\"\n",
    "DEVICE = \"mps\"  # or \"cuda\" or \"cpu\"\n",
    "DATASET = \"cifar10\"  # or \"cifar100\" or \"stl10\"\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, device=\"mps\"):\n",
    "    \"\"\"Load model from checkpoint\"\"\"\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    config = checkpoint.get('config', {})\n",
    "    model_state = checkpoint.get('model_state_dict', checkpoint.get('target_encoder', {}))\n",
    "    \n",
    "    model = create_hjepa(\n",
    "        encoder_type=config.get('model', {}).get('encoder_type', 'vit_base_patch16_224'),\n",
    "        img_size=config.get('data', {}).get('image_size', 224),\n",
    "        num_hierarchies=config.get('model', {}).get('num_hierarchies', 3),\n",
    "        predictor_depth=config.get('model', {}).get('predictor', {}).get('depth', 6),\n",
    "        predictor_heads=config.get('model', {}).get('predictor', {}).get('num_heads', 6),\n",
    "        use_rope=config.get('model', {}).get('use_rope', True),\n",
    "        use_flash_attention=config.get('model', {}).get('use_flash_attention', True),\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(model_state, strict=False)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Hierarchies: {config.get('model', {}).get('num_hierarchies', 3)}\")\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "# Load model\n",
    "model, config = load_model(CHECKPOINT_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = get_dataset(DATASET, root='../data', train=False, transform=transform)\n",
    "print(f\"✓ Loaded {len(dataset)} test samples from {DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(image):\n",
    "    \"\"\"Denormalize image for visualization\"\"\"\n",
    "    image = image.clone()\n",
    "    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image = image + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "def show_image(image, title=\"\", ax=None):\n",
    "    \"\"\"Display an image\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    image_vis = denormalize_image(image).permute(1, 2, 0).cpu().numpy()\n",
    "    ax.imshow(image_vis)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Sample Explorer\n",
    "\n",
    "Use the slider to browse through different samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(sample_idx=widgets.IntSlider(min=0, max=len(dataset)-1, step=1, value=0))\n",
    "def explore_sample(sample_idx=0):\n",
    "    image, label = dataset[sample_idx]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_input = image.unsqueeze(0).to(DEVICE)\n",
    "        embeddings = model.target_encoder.forward_hierarchical(image_input)\n",
    "    \n",
    "    # Display\n",
    "    num_hierarchies = len(embeddings)\n",
    "    fig, axes = plt.subplots(1, num_hierarchies + 1, figsize=(4 * (num_hierarchies + 1), 4))\n",
    "    \n",
    "    # Show original image\n",
    "    show_image(image, f\"Sample {sample_idx}\\nClass: {label}\", axes[0])\n",
    "    \n",
    "    # Show hierarchical representations (PCA projection)\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        B, C, H, W = emb.shape\n",
    "        emb_flat = emb.view(B, C, -1).squeeze(0)\n",
    "        \n",
    "        # PCA to 3D\n",
    "        U, S, V = torch.svd(emb_flat)\n",
    "        proj_3d = (U[:, :3].T @ emb_flat).cpu().numpy()\n",
    "        proj_grid = proj_3d.reshape(3, H, W).transpose(1, 2, 0)\n",
    "        proj_grid = (proj_grid - proj_grid.min()) / (proj_grid.max() - proj_grid.min() + 1e-8)\n",
    "        \n",
    "        axes[i + 1].imshow(proj_grid)\n",
    "        axes[i + 1].set_title(f'Hierarchy {i+1}\\n{H}x{W}')\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(sample_idx=0, layer_idx=5, head_idx=0):\n",
    "    \"\"\"Visualize attention patterns for a specific layer and head\"\"\"\n",
    "    image, label = dataset[sample_idx]\n",
    "    image_input = image.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Forward through encoder\n",
    "    encoder = model.target_encoder\n",
    "    x = encoder.vit.patch_embed(image_input)\n",
    "    \n",
    "    if hasattr(encoder.vit, 'pos_embed') and encoder.vit.pos_embed is not None:\n",
    "        x = x + encoder.vit.pos_embed\n",
    "    \n",
    "    # Forward to specified layer\n",
    "    with torch.no_grad():\n",
    "        for i, block in enumerate(encoder.vit.blocks):\n",
    "            if i == layer_idx:\n",
    "                # Extract attention\n",
    "                B, N, C = x.shape\n",
    "                qkv = block.attn.qkv(block.norm1(x))\n",
    "                qkv = qkv.reshape(B, N, 3, block.attn.num_heads, C // block.attn.num_heads)\n",
    "                qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "                q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "                \n",
    "                attn = (q @ k.transpose(-2, -1)) * block.attn.scale\n",
    "                attn = attn.softmax(dim=-1)\n",
    "                break\n",
    "            \n",
    "            x = block(x)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    show_image(image, f\"Sample {sample_idx}\", axes[0])\n",
    "    \n",
    "    # CLS token attention\n",
    "    attn_cls = attn[0, head_idx, 0, 1:].cpu().numpy()\n",
    "    grid_size = int(np.sqrt(len(attn_cls)))\n",
    "    attn_grid = attn_cls.reshape(grid_size, grid_size)\n",
    "    \n",
    "    axes[1].imshow(attn_grid, cmap='viridis')\n",
    "    axes[1].set_title(f'Attention Map\\nLayer {layer_idx}, Head {head_idx}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    image_vis = denormalize_image(image).permute(1, 2, 0).cpu().numpy()\n",
    "    attn_resized = np.array(Image.fromarray(attn_grid).resize((224, 224), Image.BILINEAR))\n",
    "    \n",
    "    axes[2].imshow(image_vis)\n",
    "    axes[2].imshow(attn_resized, cmap='jet', alpha=0.5)\n",
    "    axes[2].set_title('Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive controls\n",
    "widgets.interact(\n",
    "    visualize_attention,\n",
    "    sample_idx=widgets.IntSlider(min=0, max=len(dataset)-1, step=1, value=0),\n",
    "    layer_idx=widgets.IntSlider(min=0, max=11, step=1, value=5),\n",
    "    head_idx=widgets.IntSlider(min=0, max=11, step=1, value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Similarity Search\n",
    "\n",
    "Find similar images based on learned representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(num_samples=1000):\n",
    "    \"\"\"Extract features from dataset\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(num_samples, len(dataset))), desc=\"Extracting features\"):\n",
    "            image, _ = dataset[i]\n",
    "            image_input = image.unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            embeddings = model.target_encoder.forward_hierarchical(image_input)\n",
    "            emb = embeddings[-1]  # Highest hierarchy\n",
    "            feat = F.adaptive_avg_pool2d(emb, 1).flatten()\n",
    "            feat = F.normalize(feat, p=2, dim=0)\n",
    "            \n",
    "            features_list.append(feat.cpu())\n",
    "    \n",
    "    return torch.stack(features_list)\n",
    "\n",
    "print(\"Extracting features... (this may take a minute)\")\n",
    "features = extract_features(1000)\n",
    "print(f\"✓ Extracted {len(features)} feature vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(query_idx=0, top_k=9):\n",
    "    \"\"\"Find most similar images to query\"\"\"\n",
    "    query_feat = features[query_idx]\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = features @ query_feat\n",
    "    \n",
    "    # Get top-k (excluding query itself)\n",
    "    top_k_indices = similarities.topk(top_k + 1).indices[1:].numpy()\n",
    "    top_k_sims = similarities.topk(top_k + 1).values[1:].numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Show query\n",
    "    query_image, query_label = dataset[query_idx]\n",
    "    show_image(query_image, f\"Query (idx={query_idx})\", axes[0])\n",
    "    \n",
    "    # Show similar images\n",
    "    for i, (idx, sim) in enumerate(zip(top_k_indices, top_k_sims)):\n",
    "        image, label = dataset[idx]\n",
    "        show_image(image, f\"#{i+1} (sim={sim:.3f})\", axes[i + 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive controls\n",
    "widgets.interact(\n",
    "    find_similar_images,\n",
    "    query_idx=widgets.IntSlider(min=0, max=len(features)-1, step=1, value=0),\n",
    "    top_k=widgets.IntSlider(min=4, max=9, step=1, value=9)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick k-NN Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from train set\n",
    "train_dataset = get_dataset(DATASET, root='../data', train=True, transform=transform)\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(min(5000, len(train_dataset))), desc=\"Extracting train features\"):\n",
    "        image, label = train_dataset[i]\n",
    "        image_input = image.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        embeddings = model.target_encoder.forward_hierarchical(image_input)\n",
    "        emb = embeddings[-1]\n",
    "        feat = F.adaptive_avg_pool2d(emb, 1).flatten()\n",
    "        feat = F.normalize(feat, p=2, dim=0)\n",
    "        \n",
    "        train_features.append(feat.cpu())\n",
    "        train_labels.append(label)\n",
    "\n",
    "train_features = torch.stack(train_features)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "print(f\"✓ Extracted {len(train_features)} train features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-NN evaluation\n",
    "def knn_accuracy(k=20):\n",
    "    \"\"\"Compute k-NN accuracy\"\"\"\n",
    "    test_features = features\n",
    "    test_labels = torch.tensor([dataset[i][1] for i in range(len(features))])\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    for i in tqdm(range(len(test_features)), desc=f\"k-NN (k={k})\"):\n",
    "        # Compute similarities\n",
    "        sims = train_features @ test_features[i]\n",
    "        \n",
    "        # Get top-k\n",
    "        topk_indices = sims.topk(k).indices\n",
    "        topk_labels = train_labels[topk_indices]\n",
    "        \n",
    "        # Majority vote\n",
    "        pred = topk_labels.mode().values.item()\n",
    "        \n",
    "        if pred == test_labels[i].item():\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = 100.0 * correct / len(test_features)\n",
    "    return accuracy\n",
    "\n",
    "# Test different k values\n",
    "k_values = [1, 5, 10, 20]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    acc = knn_accuracy(k)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"k={k:2d}: {acc:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, accuracies, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('k-NN Accuracy vs k')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Features\n",
    "\n",
    "Export features for further analysis or downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features\n",
    "output_dir = Path('../results/exported_features')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'train_features': train_features,\n",
    "    'train_labels': train_labels,\n",
    "    'test_features': features,\n",
    "    'test_labels': torch.tensor([dataset[i][1] for i in range(len(features))]),\n",
    "    'config': config,\n",
    "}, output_dir / f'{DATASET}_features.pt')\n",
    "\n",
    "print(f\"✓ Features saved to {output_dir}/{DATASET}_features.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✓ Loading and inspecting H-JEPA models\n",
    "2. ✓ Visualizing hierarchical representations\n",
    "3. ✓ Analyzing attention patterns\n",
    "4. ✓ Similarity search in feature space\n",
    "5. ✓ Quick k-NN evaluation\n",
    "6. ✓ Exporting features for downstream tasks\n",
    "\n",
    "**Next steps:**\n",
    "- Run full linear probing evaluation (see `scripts/eval_linear_probe.py`)\n",
    "- Test transfer learning on other datasets (see `scripts/eval_transfer.py`)\n",
    "- Explore feature visualizations (see `scripts/visualize_features.py`)\n",
    "- Analyze attention rollout (see `scripts/visualize_attention_rollout.py`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
