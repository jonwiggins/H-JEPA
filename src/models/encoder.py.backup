"""
Vision Transformer encoders for H-JEPA.

This module implements the Context and Target encoders with EMA update mechanism.
Supports Flash Attention for 2-5x speedup in attention computation.

Flash Attention Integration:
- Uses PyTorch 2.0+ scaled_dot_product_attention for memory-efficient attention
- Automatically falls back to standard attention if Flash Attention unavailable
- Compatible with CUDA, MPS (Apple Silicon), and CPU backends
- Can be toggled via use_flash_attention parameter
"""

import math
from typing import Optional, Tuple
import warnings

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint
import timm
from einops import rearrange


# Check Flash Attention availability (requires PyTorch 2.0+)
FLASH_ATTENTION_AVAILABLE = hasattr(F, 'scaled_dot_product_attention')
if not FLASH_ATTENTION_AVAILABLE:
    warnings.warn(
        "Flash Attention not available. PyTorch 2.0+ required for optimal performance. "
        "Falling back to standard attention mechanism."
    )


class FlashAttention(nn.Module):
    """
    Flash Attention implementation using PyTorch's scaled_dot_product_attention.

    This provides memory-efficient attention computation with automatic kernel selection
    for CUDA (Flash Attention 2), MPS (Apple Metal), and CPU backends.

    Benefits:
    - 2-5x speedup compared to standard attention
    - Reduced memory footprint (O(N) instead of O(N^2) for sequence length N)
    - Automatic mixed precision support
    - No accuracy degradation

    Args:
        dim: Input dimension
        num_heads: Number of attention heads
        qkv_bias: Whether to use bias in QKV projection
        attn_drop: Attention dropout probability
        proj_drop: Output projection dropout probability
        use_flash: Whether to use Flash Attention (falls back to standard if unavailable)
    """

    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = False,
        attn_drop: float = 0.0,
        proj_drop: float = 0.0,
        use_flash: bool = True,
    ):
        super().__init__()
        assert dim % num_heads == 0, f"dim {dim} must be divisible by num_heads {num_heads}"

        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.use_flash = use_flash and FLASH_ATTENTION_AVAILABLE

        # QKV projection
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)

        # Dropout layers
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with Flash Attention.

        Args:
            x: Input tensor [B, N, C]

        Returns:
            Output tensor [B, N, C]
        """
        B, N, C = x.shape

        # Compute Q, K, V
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, num_heads, N, head_dim]

        if self.use_flash:
            # Use Flash Attention via scaled_dot_product_attention
            # This automatically selects the best kernel (FlashAttention-2 on CUDA, optimized kernels on MPS/CPU)
            # dropout_p is only applied during training
            attn_output = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=None,
                dropout_p=self.attn_drop.p if self.training else 0.0,
                is_causal=False,
            )
        else:
            # Standard attention fallback
            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            attn_output = attn @ v

        # Reshape and project output
        x = attn_output.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)

        return x


def replace_attention_with_flash(model: nn.Module, use_flash: bool = True) -> int:
    """
    Replace standard attention modules in timm ViT with Flash Attention.

    This function recursively searches for timm's Attention modules and replaces
    them with our FlashAttention implementation. The replacement preserves all
    learned weights (QKV and projection).

    Args:
        model: The model to modify (typically a timm ViT)
        use_flash: Whether to enable Flash Attention

    Returns:
        Number of attention modules replaced
    """
    replaced_count = 0

    # Recursively replace attention modules
    for name, module in model.named_children():
        # Check if this is a timm Attention module
        if module.__class__.__name__ == 'Attention':
            # Get attention parameters
            dim = module.qkv.in_features
            num_heads = module.num_heads
            qkv_bias = module.qkv.bias is not None
            attn_drop = module.attn_drop.p if hasattr(module, 'attn_drop') else 0.0
            proj_drop = module.proj_drop.p if hasattr(module, 'proj_drop') else 0.0

            # Create Flash Attention module
            flash_attn = FlashAttention(
                dim=dim,
                num_heads=num_heads,
                qkv_bias=qkv_bias,
                attn_drop=attn_drop,
                proj_drop=proj_drop,
                use_flash=use_flash,
            )

            # Copy weights from original attention module
            flash_attn.qkv.weight.data.copy_(module.qkv.weight.data)
            if qkv_bias:
                flash_attn.qkv.bias.data.copy_(module.qkv.bias.data)
            flash_attn.proj.weight.data.copy_(module.proj.weight.data)
            if module.proj.bias is not None:
                flash_attn.proj.bias.data.copy_(module.proj.bias.data)

            # Replace the module
            setattr(model, name, flash_attn)
            replaced_count += 1
        else:
            # Recursively process child modules
            replaced_count += replace_attention_with_flash(module, use_flash)

    return replaced_count


class ContextEncoder(nn.Module):
    """
    Context encoder using Vision Transformer from timm.

    Processes the visible/context patches of the input image.

    Args:
        encoder_type: Model name from timm (e.g., 'vit_base_patch16_224')
        img_size: Input image size
        pretrained: Whether to load pretrained weights
        drop_path_rate: Stochastic depth rate
        use_gradient_checkpointing: Whether to use gradient checkpointing for memory efficiency
        use_flash_attention: Whether to use Flash Attention for 2-5x speedup

    Attributes:
        vit: Vision Transformer model
        embed_dim: Embedding dimension
        num_patches: Number of patches per image
        patch_size: Size of each patch
        use_gradient_checkpointing: Flag for gradient checkpointing
        use_flash_attention: Flag for Flash Attention
    """

    def __init__(
        self,
        encoder_type: str = "vit_base_patch16_224",
        img_size: int = 224,
        pretrained: bool = False,
        drop_path_rate: float = 0.0,
        use_gradient_checkpointing: bool = False,
        use_flash_attention: bool = True,
    ):
        super().__init__()

        # Create Vision Transformer using timm
        self.vit = timm.create_model(
            encoder_type,
            pretrained=pretrained,
            img_size=img_size,
            drop_path_rate=drop_path_rate,
        )

        # Get model properties
        self.embed_dim = self.vit.embed_dim
        self.num_patches = self.vit.patch_embed.num_patches
        self.patch_size = self.vit.patch_embed.patch_size[0]

        # Remove classification head (we don't need it for JEPA)
        if hasattr(self.vit, 'head'):
            self.vit.head = nn.Identity()

        # Gradient checkpointing flag
        self.use_gradient_checkpointing = use_gradient_checkpointing

        # Flash Attention integration
        self.use_flash_attention = use_flash_attention
        if use_flash_attention:
            num_replaced = replace_attention_with_flash(self.vit, use_flash=True)
            if num_replaced > 0:
                print(f"✓ Flash Attention enabled: Replaced {num_replaced} attention modules")
                if not FLASH_ATTENTION_AVAILABLE:
                    print("  Warning: PyTorch 2.0+ not detected, using standard attention fallback")
            else:
                print("  Warning: No attention modules found to replace")

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Forward pass through the context encoder.

        Args:
            x: Input images [B, C, H, W]
            mask: Optional mask for patches [B, N] where True indicates masked patches

        Returns:
            Encoded features [B, N, D] where N is number of patches, D is embed_dim
        """
        # Get patch embeddings
        x = self.vit.patch_embed(x)

        # Add class token
        cls_token = self.vit.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_token, x), dim=1)

        # Add positional embeddings
        x = x + self.vit.pos_embed
        x = self.vit.pos_drop(x)

        # Apply mask if provided (set masked patches to zero)
        if mask is not None:
            # mask shape: [B, N], we need [B, N+1, 1] to account for cls token
            # Determine dtype for zeros tensor
            zero_dtype = (
                torch.bool if mask.dtype == torch.bool else mask.dtype
            )
            mask_with_cls = torch.cat([
                torch.zeros(
                    mask.shape[0], 1, device=mask.device, dtype=zero_dtype
                ),
                mask
            ], dim=1).unsqueeze(-1)
            # Convert boolean mask to float for multiplication
            if mask_with_cls.dtype == torch.bool:
                mask_with_cls = = mask_with_cls.float()
            x = x * (1 - mask_with_cls)

        # Pass through transformer blocks with optional gradient checkpointing
        # Gradient checkpointing trades computation for memory by recomputing
        # intermediate activations during backward pass instead of storing them
        if self.use_gradient_checkpointing and self.training:
            for block in self.vit.blocks:
                # torch.utils.checkpoint requires use_reentrant=False for compatibility
                # with distributed training and certain edge cases
                x = torch.utils.checkpoint.checkpoint(
                    block, x, use_reentrant=False
                )
        else:
            x = self.vit.blocks(x)

        x = self.vit.norm(x)

        return x

    def get_num_patches(self, img_size: int) -> int:
        """Calculate number of patches for given image size."""
        return (img_size // self.patch_size) ** 2

    def get_patch_size(self) -> int:
        """Return the patch size."""
        return self.patch_size


class TargetEncoder(nn.Module):
    """
    Target encoder with Exponential Moving Average (EMA) updates.

    Processes the full image to generate target representations.
    Updates weights via EMA from the context encoder.

    Args:
        encoder_type: Model name from timm (e.g., 'vit_base_patch16_224')
        img_size: Input image size
        ema_momentum: Initial EMA momentum (tau)
        ema_momentum_end: Final EMA momentum
        ema_warmup_steps: Number of warmup steps for EMA schedule
        pretrained: Whether to load pretrained weights
        drop_path_rate: Stochastic depth rate
        use_flash_attention: Whether to use Flash Attention for 2-5x speedup

    Attributes:
        vit: Vision Transformer model (updated via EMA)
        momentum: Current EMA momentum value
        ema_momentum_end: Target momentum value
        ema_warmup_steps: Warmup steps for momentum schedule
        use_flash_attention: Flag for Flash Attention
    """

    def __init__(
        self,
        encoder_type: str = "vit_base_patch16_224",
        img_size: int = 224,
        ema_momentum: float = 0.996,
        ema_momentum_end: float = 1.0,
        ema_warmup_steps: int = 1000,
        pretrained: bool = False,
        drop_path_rate: float = 0.0,
        use_flash_attention: bool = True,
    ):
        super().__init__()

        # Create Vision Transformer using timm
        self.vit = timm.create_model(
            encoder_type,
            pretrained=pretrained,
            img_size=img_size,
            drop_path_rate=drop_path_rate,
        )

        # Get model properties
        self.embed_dim = self.vit.embed_dim
        self.num_patches = self.vit.patch_embed.num_patches
        self.patch_size = self.vit.patch_embed.patch_size[0]

        # Remove classification head
        if hasattr(self.vit, 'head'):
            self.vit.head = nn.Identity()

        # EMA parameters
        self.momentum = ema_momentum
        self.ema_momentum_end = ema_momentum_end
        self.ema_warmup_steps = ema_warmup_steps

        # Flash Attention integration
        self.use_flash_attention = use_flash_attention
        if use_flash_attention:
            num_replaced = replace_attention_with_flash(self.vit, use_flash=True)
            if num_replaced > 0:
                print(f"✓ Flash Attention enabled (Target): Replaced {num_replaced} attention modules")

        # Disable gradient computation for target encoder
        for param in self.parameters():
            param.requires_grad = False

    @torch.no_grad()
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the target encoder.

        Args:
            x: Input images [B, C, H, W]

        Returns:
            Encoded features [B, N, D] where N is number of patches, D is embed_dim
        """
        # Get patch embeddings
        x = self.vit.patch_embed(x)

        # Add class token
        cls_token = self.vit.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_token, x), dim=1)

        # Add positional embeddings
        x = x + self.vit.pos_embed
        x = self.vit.pos_drop(x)

        # Pass through transformer blocks
        x = self.vit.blocks(x)
        x = self.vit.norm(x)

        return x

    @torch.no_grad()
    def update_from_context_encoder(
        self,
        context_encoder: ContextEncoder,
        current_step: int,
    ) -> float:
        """
        Update target encoder weights using EMA from context encoder.

        Implements linear schedule for momentum as per I-JEPA paper:
        tau(t) = tau_base + (tau_end - tau_base) * min(1.0, t / T)

        Args:
            context_encoder: Context encoder to copy weights from
            current_step: Current training step for momentum scheduling

        Returns:
            Current momentum value
        """
        # Calculate momentum with linear schedule
        progress = min(1.0, current_step / self.ema_warmup_steps)
        momentum = self.momentum + (self.ema_momentum_end - self.momentum) * progress

        # Update weights: θ_target = momentum * θ_target + (1 - momentum) * θ_context
        for param_target, param_context in zip(
            self.vit.parameters(), context_encoder.vit.parameters()
        ):
            param_target.data.mul_(momentum).add_(
                param_context.data, alpha=1 - momentum
            )

        return momentum

    @torch.no_grad()
    def copy_from_context_encoder(self, context_encoder: ContextEncoder):
        """
        Initialize target encoder with context encoder weights.

        Args:
            context_encoder: Context encoder to copy weights from
        """
        for param_target, param_context in zip(
            self.vit.parameters(), context_encoder.vit.parameters()
        ):
            param_target.data.copy_(param_context.data)


def create_encoder(
    encoder_type: str = "vit_base_patch16_224",
    img_size: int = 224,
    pretrained: bool = False,
    drop_path_rate: float = 0.0,
    use_flash_attention: bool = True,
) -> Tuple[ContextEncoder, TargetEncoder]:
    """
    Factory function to create context and target encoders.

    Args:
        encoder_type: Model name from timm
        img_size: Input image size
        pretrained: Whether to load pretrained weights
        drop_path_rate: Stochastic depth rate
        use_flash_attention: Whether to use Flash Attention for 2-5x speedup

    Returns:
        Tuple of (context_encoder, target_encoder)
    """
    context_encoder = ContextEncoder(
        encoder_type=encoder_type,
        img_size=img_size,
        pretrained=pretrained,
        drop_path_rate=drop_path_rate,
        use_flash_attention=use_flash_attention,
    )

    target_encoder = TargetEncoder(
        encoder_type=encoder_type,
        img_size=img_size,
        pretrained=pretrained,
        drop_path_rate=drop_path_rate,
        use_flash_attention=use_flash_attention,
    )

    # Initialize target encoder with context encoder weights
    target_encoder.copy_from_context_encoder(context_encoder)

    return context_encoder, target_encoder
