# H-JEPA Multi-Dataset Training Configuration with ImageNet-100
#
# This configuration demonstrates ImageNet-100 integration in a multi-dataset
# foundation model setup. ImageNet-100 provides superior training data compared
# to CIFAR-10/100 due to higher resolution (224x224 native) and more diverse images.
#
# EXPECTED PERFORMANCE IMPROVEMENT:
#   - Linear probe accuracy: +10-15% over CIFAR-only training
#   - Better feature quality due to higher resolution training
#   - Improved generalization to downstream tasks
#
# DATASET COMPOSITION:
#   - ImageNet-100: 126,689 images, 224x224, 100 classes (60% sampling weight)
#   - STL-10: 105,000 images (5k train + 100k unlabeled), 96x96, 10 classes (25% weight)
#   - CIFAR-100: 50,000 images, 32x32, 100 classes (15% weight)
#
# Total: ~280K images with weighted sampling favoring higher-resolution datasets
#
# TRAINING TIME (M1 Max):
#   - ~15-20 hours for 100 epochs
#   - ~3-4 days for 300 epochs (recommended for best results)

experiment:
  name: "imagenet100_multi_dataset"
  seed: 42
  output_dir: "results/imagenet100_foundation"
  save_frequency: 10
  eval_frequency: 5

model:
  # ViT-Small: Optimal for ImageNet-100 scale
  encoder_type: "vit_small_patch16_224"  # 22M parameters
  embed_dim: 384  # ViT-Small embedding dimension
  num_hierarchies: 3  # Hierarchical prediction at multiple scales

  # Target encoder (EMA of context encoder)
  target_encoder:
    ema_decay: 0.996  # Exponential moving average momentum
    ema_end_decay: 1.0  # Final EMA value
    ema_anneal_end_step: 400000  # Gradual annealing over training

  # Predictor network
  predictor:
    depth: 6  # Deeper predictor for complex datasets
    num_heads: 6
    mlp_ratio: 4.0
    qkv_bias: true
    dropout: 0.0  # No dropout in predictor

data:
  # MULTI-DATASET CONFIGURATION
  # Enable weighted sampling from multiple datasets
  use_multi_dataset: true

  # Dataset composition and weights
  # Weights control sampling probability (higher weight = more samples per epoch)
  datasets:
    - name: imagenet100
      weight: 0.60  # Primary dataset - highest resolution and diversity
      path: "./data/imagenet"  # Path to full ImageNet (will auto-filter to 100 classes)

    - name: stl10
      weight: 0.25  # Secondary dataset - includes valuable unlabeled data
      path: "./data"

    - name: cifar100
      weight: 0.15  # Tertiary dataset - adds diversity but lower resolution
      path: "./data"

  # Sampling strategy
  # Options: 'weighted' (use weights above), 'balanced' (equal samples per dataset), 'concat' (simple concatenation)
  sampling_strategy: "weighted"

  # Common dataset parameters
  data_path: "./data"  # Default path if not specified per-dataset
  image_size: 224  # All images resized to 224x224 (ImageNet standard)
  batch_size: 32  # Adjust based on GPU memory (16-64 range typical)
  num_workers: 6  # Parallel data loading workers (M1 Max: 6-8 optimal)

  # JEPA-optimized data augmentation
  # Unlike contrastive learning, JEPA uses minimal augmentation
  # since it learns from spatial prediction rather than instance discrimination
  transforms:
    crop_scale: [0.8, 1.0]  # Conservative random crop (80-100% of image)
    horizontal_flip: true  # Random horizontal flip (50% probability)
    color_jitter: 0.1  # Minimal color jitter (brightness/contrast/saturation)

training:
  epochs: 100  # 100 epochs for quick training, 300+ for best results
  warmup_epochs: 10  # Learning rate warmup period

  # Optimizer configuration
  optimizer: "adamw"
  lr: 0.0001  # Base learning rate (scaled with batch size)
  weight_decay: 0.04  # L2 regularization
  betas: [0.9, 0.95]  # Adam beta parameters

  # Learning rate schedule
  lr_schedule: "cosine"  # Cosine annealing
  min_lr_ratio: 0.01  # Final LR = 1% of base LR
  warmup_lr_ratio: 0.001  # Initial LR = 0.1% of base LR

  # Mixed precision training (faster on modern GPUs/Apple Silicon)
  use_amp: true

# HIERARCHICAL MASKING STRATEGY
# H-JEPA predicts multiple masked regions at different scales
masking:
  # Target masks (what we predict)
  num_target_masks: 4  # Predict 4 different masked regions
  mask_scale: [0.15, 0.20]  # Each mask covers 15-20% of image
  aspect_ratio: [0.75, 1.5]  # Mask aspect ratio range

  # Context masks (what we mask from context encoder input)
  num_context_masks: 1  # Single large context mask

  # Masking ensures the model learns spatial relationships
  # rather than just copying input features

# HIERARCHICAL LOSS CONFIGURATION
loss:
  type: "mse"  # Mean squared error for regression

  # Hierarchical loss weighting
  # H-JEPA predicts at multiple resolutions - weight them differently
  # [finest_level, middle_level, coarsest_level]
  hierarchy_weights: [1.0, 0.7, 0.5]  # Emphasize finest details

  normalize_embeddings: false  # Don't normalize embeddings

  # VICReg regularization (prevents representation collapse)
  # Ensures learned representations are:
  # 1. Invariant across augmentations (variance)
  # 2. Spread out in embedding space (std)
  # 3. Decorrelated (covariance)
  use_vicreg: true
  vicreg:
    sim_coeff: 25.0  # Similarity/invariance term
    std_coeff: 25.0  # Variance term (prevent collapse)
    cov_coeff: 1.0   # Covariance term (decorrelation)

# LOGGING AND MONITORING
logging:
  use_wandb: false  # Set to true if using Weights & Biases
  use_tensorboard: true  # TensorBoard for visualization
  log_frequency: 50  # Log metrics every 50 steps

  # What to log
  log_images: true  # Log sample images and masks
  log_attention: true  # Log attention maps (for visualization)
  log_gradients: false  # Log gradient norms (useful for debugging)
  log_dataset_distribution: true  # Track which datasets are sampled

# DEVICE CONFIGURATION
device: "mps"  # M1 Max MPS acceleration (use "cuda" for NVIDIA GPUs, "cpu" for CPU-only)

# CHECKPOINTING
checkpoint:
  save_best: true  # Save best checkpoint based on validation metric
  metric: "val_loss"  # Metric to monitor
  mode: "min"  # Minimize validation loss
  keep_last_k: 3  # Keep last 3 checkpoints

# NOTES FOR USERS:
#
# 1. DATASET PATHS:
#    - ImageNet-100 uses full ImageNet structure but automatically filters to 100 classes
#    - Directory structure should be:
#      data/imagenet/train/n01440764/image1.JPEG (synset ID directories)
#      data/imagenet/val/n01440764/image1.JPEG
#
# 2. MEMORY REQUIREMENTS:
#    - Batch size 32 with ViT-Small: ~8-10GB GPU memory
#    - Reduce batch_size to 16 or 8 if you encounter OOM errors
#    - Increase batch_size to 64+ if you have >16GB GPU memory
#
# 3. TRAINING TIPS:
#    - Start with 100 epochs to validate the setup
#    - For best results, train for 300-500 epochs
#    - Monitor the dataset distribution in logs to ensure balanced sampling
#    - ImageNet-100 should contribute ~60% of batches based on weight
#
# 4. EXPECTED RESULTS (100 epochs):
#    - Linear probe accuracy: 60-70% on ImageNet-100 validation
#    - k-NN accuracy: 55-65%
#    - Better than CIFAR-10 baseline by 10-15%
#
# 5. SCALING TO FULL IMAGENET:
#    - Change dataset name from 'imagenet100' to 'imagenet'
#    - Increase epochs to 300-500
#    - Expected linear probe: 70-80%
#    - Training time: ~5-7 days on M1 Max
