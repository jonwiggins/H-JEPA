# H-JEPA 8-Hour Conservative Training Configuration
#
# APPROACH: Proven Phase 1 optimizations only - maximize stability
# GOAL: Validate that Phase 1 optimizations (Flash Attention, LayerScale) work correctly
# EXPECTED TIME: 7-8 hours on M1 Max (32GB RAM, MPS backend)
# TARGET PERFORMANCE: 60-70% linear probe accuracy
#
# OPTIMIZATION STRATEGY:
# - Enable Flash Attention (2-5x speedup)
# - Enable LayerScale (training stability)
# - Use CIFAR-10 + STL-10 (fast iteration, proven dataset)
# - ViT-Small (good capacity without excessive compute)
# - Moderate epochs (50) for meaningful convergence
#
# RISK ASSESSMENT: LOW
# - All features are well-tested independently
# - Conservative hyperparameters
# - Proven dataset combination
# - Clear fallback if issues arise

experiment:
  name: "overnight_conservative_phase1"
  seed: 42
  output_dir: "results/overnight_conservative"
  save_frequency: 10
  eval_frequency: 10
  description: "Conservative 8h training with Flash Attention + LayerScale"

# Model configuration - ViT-Small with Phase 1 optimizations
model:
  # ViT-Small: 22M params, good balance of speed and capacity
  encoder_type: "vit_small_patch16_224"
  embed_dim: 384
  num_hierarchies: 3  # Standard 3-level hierarchy

  # PHASE 1 OPTIMIZATION: Flash Attention (ENABLED)
  # Expected: 2-3x speedup, 30-40% memory reduction
  # Risk: LOW - PyTorch 2.0+ native support
  use_flash_attention: true

  # PHASE 1 OPTIMIZATION: LayerScale (ENABLED)
  # Expected: +0.5-1.0% accuracy, better training stability
  # Risk: LOW - proven technique from DeiT III
  use_layerscale: true
  layerscale_init: 1e-5  # DeiT III default

  # Gradient checkpointing: DISABLED for conservative run
  # (can enable if memory is tight, trades speed for memory)
  use_gradient_checkpointing: false

  # Advanced features: DISABLED for conservative run
  use_fpn: false

  rope:
    use_rope: false  # Keep standard position embeddings

  # Predictor configuration
  predictor:
    depth: 6
    num_heads: 6
    mlp_ratio: 4.0
    qkv_bias: true
    dropout: 0.0

  # Target encoder (EMA)
  ema:
    momentum: 0.996
    momentum_end: 1.0
    momentum_warmup_epochs: 10

# Dataset: Multi-dataset with CIFAR-10 + STL-10
data:
  # Use multi-dataset for diversity without complexity
  use_multi_dataset: true

  datasets:
    - name: cifar10
      weight: 0.5  # 50% CIFAR-10
      path: "./data"

    - name: stl10
      weight: 0.5  # 50% STL-10
      path: "./data"

  sampling_strategy: "weighted"
  image_size: 224

  # Batch size: Conservative for M1 Max
  # 32 should fit comfortably with Flash Attention enabled
  batch_size: 32

  num_workers: 4  # Conservative worker count
  pin_memory: false  # Not needed for MPS

  # Standard JEPA augmentation (minimal)
  transforms:
    crop_scale: [0.8, 1.0]
    horizontal_flip: true
    color_jitter: 0.1

# Training hyperparameters - Conservative and stable
training:
  # 50 epochs: Balance between time and convergence
  # Expected: ~9-10 min/epoch = 7.5-8 hours total
  epochs: 50

  warmup_epochs: 5

  # Learning rate: Conservative baseline
  # Not increasing yet - validate optimizations first
  optimizer: "adamw"
  lr: 0.00015  # Slightly higher than baseline
  weight_decay: 0.04
  betas: [0.9, 0.95]

  # Learning rate schedule
  lr_schedule: "cosine"
  min_lr_ratio: 0.01
  warmup_lr_ratio: 0.001

  # Gradient clipping for stability
  clip_grad: 3.0

  # Mixed precision training
  use_amp: true

  # No gradient accumulation needed with batch_size=32
  accumulation_steps: 1

# Masking strategy - Standard H-JEPA
masking:
  num_masks: 4
  mask_scale: [0.15, 0.2]
  aspect_ratio: [0.75, 1.5]
  num_context_masks: 1
  context_scale: [0.85, 1.0]

# Loss configuration - Standard with VICReg
loss:
  type: "smoothl1"  # Smoother than MSE

  # Hierarchical weights
  hierarchy_weights: [1.0, 0.7, 0.5]

  normalize_embeddings: false

  # VICReg regularization (prevent collapse)
  use_vicreg: true
  vicreg:
    sim_coeff: 25.0
    std_coeff: 25.0
    cov_coeff: 1.0

  # NO contrastive component for conservative run
  use_contrastive: false

# Logging
logging:
  use_wandb: false
  use_tensorboard: true
  log_frequency: 50

  log_images: true
  log_attention: false  # Disable for speed
  log_gradients: false
  log_dataset_distribution: true

# Device
device: "mps"  # M1 Max acceleration

# Checkpointing
checkpoint:
  save_best: true
  metric: "val_loss"
  mode: "min"
  keep_last_k: 3

# Evaluation during training
evaluation:
  eval_frequency: 10

  # Quick k-NN evaluation during training
  knn:
    enabled: true
    k_values: [1, 5, 20]
    frequency: 10

# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# TRAINING TIME:
#   - Per epoch: ~9-10 minutes (with Flash Attention speedup)
#   - Total: 50 epochs × 9.5 min = 7.5-8 hours
#
# PERFORMANCE TARGETS:
#   - Linear probe accuracy: 60-70%
#   - k-NN accuracy (k=20): 55-65%
#   - Final loss: < 0.5
#   - No representation collapse
#
# MEMORY USAGE:
#   - Peak: 12-16 GB RAM (well within 32GB limit)
#   - GPU (MPS): 8-10 GB
#
# VALIDATION CHECKPOINTS:
#   - Epoch 10: Loss should be decreasing steadily
#   - Epoch 25: k-NN should show 40-50% accuracy
#   - Epoch 40: Near-final performance visible
#   - Epoch 50: Final evaluation
#
# SUCCESS CRITERIA:
#   ✓ Training completes in < 8.5 hours
#   ✓ No NaN/Inf losses
#   ✓ Flash Attention shows speedup vs baseline
#   ✓ LayerScale shows stable training (smooth loss curves)
#   ✓ Linear probe > 60%
#   ✓ Feature variance > 0.1 (no collapse)
#
# =============================================================================
# FALLBACK PLAN
# =============================================================================
#
# If training breaks:
#
# 1. Out of Memory:
#    - Reduce batch_size: 32 → 24 → 16
#    - Enable gradient_checkpointing: true
#    - Reduce num_workers: 4 → 2
#
# 2. Flash Attention errors:
#    - Set use_flash_attention: false
#    - Training will be slower but should work
#    - Expected time: 12-14 hours instead
#
# 3. Training instability (NaN/divergence):
#    - Reduce learning rate: 0.00015 → 0.0001
#    - Increase warmup: 5 → 10 epochs
#    - Increase gradient clipping: 3.0 → 1.0
#
# 4. Too slow (> 12 min/epoch):
#    - Reduce epochs: 50 → 40 → 30
#    - Reduce num_workers: 4 → 2
#    - Disable log_images and log_dataset_distribution
#
# =============================================================================
# MONITORING CHECKLIST
# =============================================================================
#
# Watch these metrics during training:
#
# Every 10 epochs:
#   □ Training loss decreasing smoothly
#   □ Feature variance > 0.1
#   □ Effective rank > 100 (50% of 384 dims)
#   □ k-NN accuracy improving
#   □ No NaN/Inf values
#   □ Memory usage < 20 GB
#
# Epoch 25 checkpoint:
#   □ Loss < 1.0 (should be much lower, ~0.3-0.5)
#   □ k-NN accuracy > 40%
#   □ Attention patterns look reasonable
#
# Final (Epoch 50):
#   □ Run full linear probe evaluation
#   □ Compare with baseline (if available)
#   □ Check Flash Attention speedup in logs
#   □ Document any issues for aggressive run
#
# =============================================================================
