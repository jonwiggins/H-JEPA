experiment:
  name: foundation_mini
  seed: 42
  output_dir: results/foundation_model
  save_frequency: 10
  eval_frequency: 10
model:
  encoder_type: vit_small_patch16_224
  embed_dim: 384
  num_hierarchies: 3
  target_encoder:
    ema_decay: 0.996
    ema_end_decay: 1.0
    ema_anneal_end_step: 300000
  predictor:
    depth: 6
    num_heads: 6
    mlp_ratio: 4.0
    qkv_bias: true
    dropout: 0.0
data:
  use_multi_dataset: true
  datasets:
  - name: imagenet100
    weight: 0.6000000000000001
  - name: stl10
    weight: 0.30000000000000004
  - name: cifar100
    weight: 0.10000000000000002
  sampling_strategy: weighted
  data_path: ./data
  image_size: 224
  batch_size: 32
  num_workers: 6
  transforms:
    crop_scale:
    - 0.8
    - 1.0
    horizontal_flip: true
    color_jitter: 0.1
training:
  epochs: 100
  warmup_epochs: 10
  optimizer: adamw
  lr: 0.0001
  weight_decay: 0.04
  betas:
  - 0.9
  - 0.95
  lr_schedule: cosine
  min_lr_ratio: 0.01
  warmup_lr_ratio: 0.001
  use_amp: true
masking:
  num_target_masks: 4
  mask_scale:
  - 0.15
  - 0.2
  aspect_ratio:
  - 0.75
  - 1.5
  num_context_masks: 1
loss:
  type: mse
  hierarchy_weights:
  - 1.0
  - 0.7
  - 0.5
  normalize_embeddings: false
  use_vicreg: true
  vicreg:
    sim_coeff: 25.0
    std_coeff: 25.0
    cov_coeff: 1.0
logging:
  use_wandb: false
  use_tensorboard: true
  log_frequency: 50
  log_images: true
  log_attention: true
  log_gradients: false
  log_dataset_distribution: true
device: mps
checkpoint:
  save_best: true
  metric: val_loss
  mode: min
  keep_last_k: 3
