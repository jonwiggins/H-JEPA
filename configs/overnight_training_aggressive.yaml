# H-JEPA 8-Hour Aggressive Training Configuration
#
# APPROACH: Enable most Phase 1-3 optimizations - push performance boundaries
# GOAL: Test advanced optimizations and maximize performance in 8 hours
# EXPECTED TIME: 7-8 hours on M1 Max (32GB RAM, MPS backend)
# TARGET PERFORMANCE: 70-78% linear probe accuracy
#
# OPTIMIZATION STRATEGY:
# - Phase 1: Flash Attention + LayerScale (proven)
# - Phase 2: ImageNet-100 dataset (higher quality, native 224x224)
# - Phase 3: FPN + Contrastive component (advanced features)
# - Optimized hyperparameters (higher LR, gradient accumulation)
#
# RISK ASSESSMENT: MEDIUM-HIGH
# - Multiple new features combined
# - More complex dataset (ImageNet-100)
# - Aggressive hyperparameters
# - Higher chance of issues, but higher potential reward

experiment:
  name: "overnight_aggressive_phase123"
  seed: 42
  output_dir: "results/overnight_aggressive"
  save_frequency: 5
  eval_frequency: 5
  description: "Aggressive 8h training with Phase 1-3 optimizations"

# Model configuration - ViT-Small with ALL viable optimizations
model:
  # ViT-Small: 22M params
  encoder_type: "vit_small_patch16_224"
  embed_dim: 384
  num_hierarchies: 3

  # PHASE 1 OPTIMIZATION: Flash Attention (ENABLED)
  # Expected: 2-5x speedup, critical for fitting in 8 hours
  use_flash_attention: true

  # PHASE 1 OPTIMIZATION: LayerScale (ENABLED)
  # Expected: Better stability with aggressive hyperparameters
  use_layerscale: true
  layerscale_init: 1e-5

  # PHASE 2 OPTIMIZATION: Gradient Checkpointing (ENABLED)
  # Trade 15-20% speed for 2x batch size via lower memory
  # Net benefit: Better convergence with larger effective batch
  use_gradient_checkpointing: true

  # PHASE 3 OPTIMIZATION: Feature Pyramid Network (ENABLED)
  # Expected: +1-2% on downstream tasks, better multi-scale features
  use_fpn: true
  fpn:
    feature_dim: 384  # Same as embed_dim
    fusion_method: "add"  # Faster than concat

  # RoPE: DISABLED for this run
  # (would add ~10% overhead, save for future iteration)
  rope:
    use_rope: false

  # Predictor configuration - slightly deeper for FPN
  predictor:
    depth: 6
    num_heads: 6
    mlp_ratio: 4.0
    qkv_bias: true
    dropout: 0.0

  # Target encoder (EMA) - faster annealing for 40 epochs
  ema:
    momentum: 0.996
    momentum_end: 1.0
    momentum_warmup_epochs: 8  # Faster warmup

# PHASE 2 OPTIMIZATION: Dataset - ImageNet-100
# Major improvement: Native 224x224 resolution vs upscaled 32x32
# Expected: +10-15% linear probe vs CIFAR
data:
  # Single high-quality dataset for this run
  # (Multi-dataset adds complexity - test separately)
  use_multi_dataset: false

  dataset: "imagenet100"
  data_path: "./data/imagenet"
  image_size: 224

  # PHASE 2 OPTIMIZATION: Larger effective batch via gradient accumulation
  # Physical batch: 16 (fits in memory with grad checkpointing)
  # Accumulation: 4 steps
  # Effective batch: 64 (optimal for convergence)
  batch_size: 16

  num_workers: 6  # More workers for ImageNet loading
  pin_memory: false  # MPS doesn't benefit

  # PHASE 1 OPTIMIZATION: DeiT III-inspired augmentation
  # Stronger augmentation for better generalization
  augmentation:
    strategy: "deit3"  # If implemented, otherwise falls back to standard

    # Standard augmentation (fallback if deit3 not available)
    crop_scale: [0.4, 1.0]  # More aggressive crops
    horizontal_flip: true
    color_jitter: 0.4  # Stronger than conservative

    # DeiT III specific (if available)
    deit3:
      auto_augment: true
      rand_aug_num_ops: 2
      rand_aug_magnitude: 9
      random_erasing_prob: 0.1  # Light erasing for SSL
      mixup_alpha: 0.0  # Disable mixup for SSL
      cutmix_alpha: 0.0  # Disable cutmix for SSL

# Training hyperparameters - AGGRESSIVE
training:
  # PHASE 2 OPTIMIZATION: Fewer epochs but better quality
  # 40 epochs with ImageNet-100 > 50 epochs with CIFAR
  # Expected: ~10-12 min/epoch = 6.5-8 hours total
  epochs: 40

  warmup_epochs: 4  # Fast warmup (10% of training)

  # PHASE 2 OPTIMIZATION: Higher learning rate
  # Scale with effective batch size (64)
  # Base LR: 0.0001 for batch 32
  # Scaled LR: 0.0002 for batch 64 (linear scaling)
  optimizer: "adamw"
  lr: 0.0003  # Even more aggressive - testing limits
  weight_decay: 0.05  # Slightly higher for regularization
  betas: [0.9, 0.95]

  # Learning rate schedule
  lr_schedule: "cosine"
  min_lr_ratio: 0.01
  warmup_lr_ratio: 0.001

  # Gradient clipping - tighter for aggressive LR
  clip_grad: 1.0

  # Mixed precision training
  use_amp: true

  # PHASE 2 OPTIMIZATION: Gradient accumulation
  accumulation_steps: 4  # Effective batch = 64

# Masking strategy - Slightly more aggressive
masking:
  num_masks: 4
  mask_scale: [0.15, 0.25]  # Wider range
  aspect_ratio: [0.6, 1.5]  # Wider aspect ratios
  num_context_masks: 1
  context_scale: [0.85, 1.0]

# PHASE 3 OPTIMIZATION: Hybrid loss with contrastive component
loss:
  # Use combined JEPA + Contrastive loss
  type: "cjepa"  # If available, otherwise 'smoothl1'

  # JEPA component
  jepa_loss_type: "smoothl1"
  hierarchy_weights: [1.0, 0.8, 0.6]  # More balanced weights for FPN
  normalize_embeddings: true  # Required for contrastive
  jepa_weight: 1.0

  # VICReg regularization
  use_vicreg: true
  vicreg:
    sim_coeff: 25.0
    std_coeff: 25.0
    cov_coeff: 1.0

  # PHASE 3 OPTIMIZATION: Contrastive component
  # Expected: +0.8-1.0% improvement
  # Weight: 10% of total loss
  use_contrastive: true
  contrastive_weight: 0.1
  contrastive_temperature: 0.1
  use_cosine_similarity: true
  contrastive_on_context: false  # Apply to target encoder

# Logging - More frequent for aggressive run
logging:
  use_wandb: false
  use_tensorboard: true
  log_frequency: 25  # More frequent than conservative

  log_images: true
  log_attention: true  # Enable for FPN visualization
  log_gradients: true  # Monitor for instability
  log_dataset_distribution: false  # Single dataset

# Device
device: "mps"

# Checkpointing - More frequent for aggressive run
checkpoint:
  save_best: true
  metric: "val_loss"
  mode: "min"
  keep_last_k: 5  # Keep more checkpoints

# Evaluation during training
evaluation:
  eval_frequency: 5  # Every 5 epochs

  knn:
    enabled: true
    k_values: [1, 5, 20]
    frequency: 5

# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# TRAINING TIME:
#   - Per epoch: 10-12 minutes (ImageNet-100 is slower than CIFAR)
#   - Total: 40 epochs × 11 min = 7.3 hours
#   - Buffer: +0.5-1 hour for evaluations = 7.8-8.3 hours
#
# PERFORMANCE TARGETS (Optimistic):
#   - Linear probe accuracy: 70-78% (ImageNet-100)
#   - k-NN accuracy (k=20): 65-73%
#   - Final loss: < 0.3
#   - Strong multi-scale features (FPN)
#   - Better instance discrimination (contrastive)
#
# PERFORMANCE TARGETS (Realistic):
#   - Linear probe accuracy: 65-72%
#   - k-NN accuracy (k=20): 60-68%
#   - Visible improvement over conservative config
#
# MEMORY USAGE:
#   - Peak: 18-24 GB RAM (gradient checkpointing helps)
#   - GPU (MPS): 10-14 GB
#   - Should fit in 32 GB with headroom
#
# SPEEDUP FROM OPTIMIZATIONS:
#   - Flash Attention: 2-3x over baseline
#   - Gradient Checkpointing: -15% speed, +2x batch size (net positive)
#   - Higher LR: Faster convergence (fewer epochs needed)
#   - Net result: Similar time to conservative but better dataset
#
# =============================================================================
# SUCCESS CRITERIA
# =============================================================================
#
# Must Have (Critical):
#   ✓ Training completes in < 9 hours
#   ✓ No catastrophic failures (NaN/Inf)
#   ✓ Linear probe > 60% (better than CIFAR baseline)
#   ✓ All optimizations run without errors
#
# Should Have (Expected):
#   ✓ Linear probe > 68%
#   ✓ Visible FPN benefit in attention maps
#   ✓ Contrastive accuracy > 0.9
#   ✓ Smooth training (no oscillations)
#
# Nice to Have (Stretch):
#   ✓ Linear probe > 73%
#   ✓ Match or exceed I-JEPA baseline
#   ✓ Clear hierarchy differentiation
#   ✓ Strong transfer to downstream tasks
#
# =============================================================================
# RISK MITIGATION
# =============================================================================
#
# High-Risk Elements:
#
# 1. ImageNet-100 Loading:
#    - RISK: Dataset path issues, missing files
#    - MITIGATION: Verify dataset exists before training
#    - FALLBACK: Switch to imagenet100_multi_dataset.yaml config
#
# 2. Multiple New Features:
#    - RISK: Interaction bugs between FPN + Contrastive + Flash Attention
#    - MITIGATION: Each tested independently before
#    - FALLBACK: Disable features one at a time if issues
#
# 3. Aggressive Hyperparameters:
#    - RISK: Training instability, divergence
#    - MITIGATION: Monitor gradients closely
#    - FALLBACK: Reduce LR to 0.0002 or 0.00015
#
# 4. Memory Issues:
#    - RISK: OOM with gradient checkpointing + FPN
#    - MITIGATION: Gradient checkpointing enabled
#    - FALLBACK: Reduce batch_size to 12 or 8
#
# =============================================================================
# FALLBACK PLAN (If training breaks)
# =============================================================================
#
# Priority 1 - Quick Fixes (try first):
#   □ Reduce LR: 0.0003 → 0.0002 → 0.00015
#   □ Reduce batch_size: 16 → 12 → 8
#   □ Increase accumulation_steps: 4 → 6 → 8
#   □ Reduce epochs: 40 → 35 → 30
#
# Priority 2 - Disable Features (if still broken):
#   □ Disable contrastive: use_contrastive: false
#   □ Disable FPN: use_fpn: false
#   □ Disable gradient checkpointing: use_gradient_checkpointing: false
#   □ Increase batch_size back to 24 or 32
#
# Priority 3 - Dataset Fallback (last resort):
#   □ Switch to CIFAR+STL multi-dataset
#   □ Use conservative config as base
#   □ Add only Flash Attention + LayerScale
#
# =============================================================================
# MONITORING GUIDE
# =============================================================================
#
# Critical Metrics (Check every epoch):
#   □ Training loss: Should decrease smoothly
#   □ Gradient norm: Should be < 10 (clip_grad=1.0)
#   □ Memory usage: Should stay < 25 GB
#   □ Time per epoch: Should be 10-12 minutes
#
# Early Warning Signs (Stop if seen):
#   □ Loss increases for 3+ consecutive epochs
#   □ NaN or Inf in any metric
#   □ Gradient norm > 100 (exploding gradients)
#   □ Memory usage > 28 GB (approaching limit)
#   □ Epoch time > 15 minutes (too slow)
#
# Phase-Specific Checks:
#
#   Epoch 5:
#     □ Loss < 2.0 (should be ~0.5-1.0)
#     □ Contrastive accuracy > 0.7
#     □ No memory issues
#     □ FPN features look reasonable
#
#   Epoch 15:
#     □ Loss < 0.8
#     □ k-NN accuracy > 45%
#     □ Contrastive accuracy > 0.85
#     □ Hierarchy weights balanced
#
#   Epoch 30:
#     □ Loss < 0.4
#     □ k-NN accuracy > 60%
#     □ Near-final performance
#     □ Prepare for final evaluation
#
#   Epoch 40 (Final):
#     □ Run full linear probe
#     □ Evaluate on multiple k values
#     □ Check feature quality metrics
#     □ Compare with conservative config
#
# =============================================================================
# POST-TRAINING ANALYSIS
# =============================================================================
#
# Questions to Answer:
#
# 1. Performance:
#    - Did we beat conservative config?
#    - By how much? (target: +5-10% linear probe)
#    - Was the extra complexity worth it?
#
# 2. Features:
#    - Flash Attention speedup: Measure actual vs expected
#    - LayerScale impact: Compare training curves
#    - FPN benefit: Check multi-scale feature quality
#    - Contrastive benefit: Measure accuracy improvement
#
# 3. Efficiency:
#    - Time per epoch: Compare with/without optimizations
#    - Memory usage: Gradient checkpointing impact
#    - Throughput: Images/second with optimizations
#
# 4. Stability:
#    - Any issues during training?
#    - Which features caused problems (if any)?
#    - What would we change for next run?
#
# 5. Next Steps:
#    - Which features to keep?
#    - Which to remove or tune?
#    - Ready for longer training (100+ epochs)?
#    - Worth scaling to full ImageNet?
#
# =============================================================================
