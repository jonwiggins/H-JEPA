experiment:
  name: foundation_cifar_stl
  seed: 42
  output_dir: results/foundation_model
  save_frequency: 10
  eval_frequency: 10

model:
  encoder_type: vit_small_patch16_224
  embed_dim: 384
  num_hierarchies: 3
  ema:
    momentum: 0.996
    momentum_end: 1.0
    momentum_warmup_epochs: 10
  predictor:
    depth: 6
    num_heads: 6
    mlp_ratio: 4.0
    qkv_bias: true
    dropout: 0.0

data:
  use_multi_dataset: true
  datasets:
    - name: stl10
      weight: 0.50
    - name: cifar100
      weight: 0.30
    - name: cifar10
      weight: 0.20
  sampling_strategy: weighted
  data_path: ./data
  image_size: 224
  batch_size: 32
  num_workers: 6
  transforms:
    crop_scale:
      - 0.8
      - 1.0
    horizontal_flip: true
    color_jitter: 0.1

training:
  epochs: 100
  warmup_epochs: 10
  optimizer: adamw
  lr: 0.0001
  min_lr: 1.0e-6
  weight_decay: 0.04
  betas:
    - 0.9
    - 0.95
  lr_schedule: cosine
  min_lr_ratio: 0.01
  warmup_lr_ratio: 0.001
  use_amp: true

masking:
  num_target_masks: 4
  mask_scale:
    - 0.15
    - 0.25
  aspect_ratio:
    - 0.75
    - 1.5
  num_context_masks: 1

loss:
  type: smoothl1
  hierarchy_weights:
    - 1.0
    - 0.7
    - 0.5
  use_vicreg: true
  vicreg:
    sim_coeff: 25.0
    std_coeff: 25.0
    cov_coeff: 1.0

logging:
  experiment_name: foundation_cifar_stl
  log_dir: results/foundation_model/logs
  use_wandb: false
  use_tensorboard: true
  log_frequency: 50
  log_images: true
  log_attention: true
  log_gradients: false
  log_dataset_distribution: true

device: mps

checkpoint:
  checkpoint_dir: results/foundation_model/checkpoints
  save_frequency: 10
  save_best: true
  metric: val_loss
  mode: min
  keep_last_k: 3
