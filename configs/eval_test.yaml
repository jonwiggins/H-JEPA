# Minimal configuration for evaluation testing
# Quick training to generate a checkpoint for evaluation

# Experiment settings
experiment:
  name: "eval_test"
  seed: 42
  output_dir: "results/eval_test"

# Model configuration - ViT-Tiny for speed
model:
  encoder_type: "vit_tiny_patch16_224"
  img_size: 224
  num_hierarchies: 3
  embed_dim: 192
  use_fpn: true
  fpn_feature_dim: 128
  use_rope: false
  use_flash_attention: false
  use_mps_optimization: true
  use_layerscale: false
  predictor_depth: 4
  predictor_mlp_ratio: 4.0
  predictor_use_norm: true

# Data configuration
data:
  dataset: "cifar10"
  data_path: "./data"
  batch_size: 16
  num_workers: 2
  image_size: 224
  transforms:
    train:
      RandomResizedCrop:
        size: 224
        scale: [0.4, 1.0]
      RandomHorizontalFlip:
        p: 0.5
      Normalize:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]
    val:
      Resize:
        size: 256
      CenterCrop:
        size: 224
      Normalize:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

# Masking configuration
masking:
  num_target_masks: 4
  target_mask_scale: [0.15, 0.4]
  target_aspect_ratio: [0.5, 2.0]
  num_context_masks: 2
  context_mask_scale: [0.7, 1.0]
  hierarchical_masking: true
  mask_at_levels: [1, 2, 3]
  mask_overlap_threshold: 0.1

# Loss configuration
loss:
  type: "combined"
  smoothl1_beta: 0.01
  hierarchy_weights: [1.0, 0.8, 0.6]
  use_vicreg: false
  use_sigreg: false

# Training configuration - Just 10 steps for checkpoint
training:
  epochs: 1
  warmup_epochs: 0
  learning_rate: 1.5e-4
  lr: 1.5e-4
  weight_decay: 0.04
  betas: [0.9, 0.95]
  eps: 1.0e-8
  optimizer: "adamw"
  scheduler: "cosine"
  lr_schedule: "cosine"
  scheduler_params:
    min_lr: 1.5e-4
    warmup_start_lr: 1.5e-4
  gradient_clip: 1.0
  use_amp: false
  use_gradient_checkpointing: false
  use_compile: false
  accumulation_steps: 1
  ema_momentum_schedule:
    start: 0.996
    end: 1.0
    warmup_steps: 100
  clear_cuda_cache_freq: 50

# Logging configuration
logging:
  log_frequency: 5
  save_frequency: 10  # Save checkpoint after 10 steps
  use_wandb: false
  use_tensorboard: false
  log_images: false
  log_gradients: false
  log_memory: false
  log_metrics: true

# Evaluation configuration
evaluation:
  eval_frequency: 50
  eval_linear_probe: false
  eval_knn: false

# Device configuration
device: "mps"

# Checkpointing
checkpoint:
  save_best: false
  save_last: true
  max_checkpoints: 1
  metric: "train_loss"
  mode: "min"
  resume: null
