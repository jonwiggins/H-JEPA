# H-JEPA Full Training - 100 Epochs on M1 Max
# Extended training for competitive performance
# Expected training time: ~12-13 hours
# Target: 80-85% linear probe accuracy on CIFAR-10

# Model architecture - ViT-Small for better capacity
model:
  # ViT-Small - more capacity for 100-epoch training
  # Note: ~22M params, expect ~2.0-2.5 it/s on M1 Max
  encoder_type: "vit_small_patch16_224"
  embed_dim: 384
  num_hierarchies: 3  # 3 levels for richer hierarchy

  # Predictor configuration
  predictor:
    depth: 6  # Full depth
    num_heads: 12
    mlp_ratio: 4.0

  # EMA settings for long training
  ema:
    momentum: 0.996
    momentum_end: 1.0
    momentum_warmup_epochs: 30  # Gradual warmup

# Dataset and data loading
data:
  dataset: "cifar10"
  data_path: "./data"
  image_size: 224

  # Batch size - may need to reduce to 24 for ViT-Small
  batch_size: 24

  num_workers: 4
  pin_memory: false

  # Full augmentation for 100 epochs
  augmentation:
    color_jitter: 0.4
    horizontal_flip: true
    random_crop: true

# Masking strategy
masking:
  num_masks: 4
  mask_scale: [0.15, 0.2]  # Standard JEPA scale
  aspect_ratio: [0.75, 1.5]
  num_context_masks: 1
  context_scale: [0.85, 1.0]

# Training hyperparameters
training:
  epochs: 100

  # Warmup
  warmup_epochs: 10

  # Learning rate
  lr: 1.5e-4

  min_lr: 1.0e-6

  # Weight decay
  weight_decay: 0.05

  # Optimizer
  optimizer: "adamw"
  betas: [0.9, 0.95]

  # Cosine schedule
  lr_schedule: "cosine"

  # Gradient clipping
  clip_grad: 3.0

  # Mixed precision
  use_amp: true

  accumulation_steps: 1

# Loss function
loss:
  type: "mse"
  hierarchy_weights: [1.0, 0.5, 0.25]  # 3 levels
  normalize_embeddings: false
  vicreg_weight: 0.1

# Checkpointing
checkpoint:
  save_frequency: 10
  keep_best_n: 3
  checkpoint_dir: "results/checkpoints"
  resume: null

# Logging
logging:
  experiment_name: "m1_max_full_100epoch"
  log_dir: "results/logs"
  log_frequency: 50

  wandb:
    enabled: false
    project: "h-jepa"
    entity: null
    tags: ["m1-max", "100-epoch", "vit-small", "cifar10"]

  tensorboard:
    enabled: true

# Distributed
distributed:
  enabled: false
  backend: "gloo"
  world_size: 1

# Evaluation
evaluation:
  eval_frequency: 10
  linear_probe:
    enabled: false

# Reproducibility
seed: 42

# Device
device: "mps"

# Expected outcomes (100 epochs):
# - Training time: ~12-13 hours (overnight)
# - Final loss: ~0.001-0.002
# - Linear probe: 80-85% accuracy
# - k-NN: 78-82% accuracy
# - Competitive with published SSL baselines
