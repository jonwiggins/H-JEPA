# =============================================================================
# H-JEPA SAFE OVERNIGHT TRAINING CONFIGURATION
# =============================================================================
#
# GOAL: Guaranteed-to-run configuration using ONLY verified working features
# EXPECTED TIME: 7-8 hours on M1 Max (32GB RAM, MPS backend)
# TARGET PERFORMANCE: 55-65% linear probe accuracy
#
# PHILOSOPHY: Reliability over performance. Better to get results than crash.
#
# =============================================================================
# WHAT'S DISABLED AND WHY
# =============================================================================
#
# ❌ Flash Attention (use_flash_attention: false)
#    REASON: Parameter accepted by HJEPA but NOT by create_encoder()
#    IMPACT: TypeError when creating encoders
#    FIX: Code needs to be updated to pass this to encoder
#
# ❌ LayerScale (removed entirely)
#    REASON: Parameter accepted by HJEPA but NOT by create_encoder()
#    IMPACT: TypeError when creating encoders
#    FIX: Code needs to be updated to pass this to encoder
#
# ❌ DeiT III Augmentation
#    REASON: Implemented but not integrated into dataset pipeline
#    IMPACT: Would need custom dataset wrapper
#    STATUS: Use standard JEPA transforms instead
#
# ❌ C-JEPA Multi-view
#    REASON: Requires 2-view training loop, different from H-JEPA
#    IMPACT: Needs separate trainer implementation
#    STATUS: Use standard H-JEPA loss
#
# =============================================================================
# WHAT'S ENABLED (VERIFIED WORKING)
# =============================================================================
#
# ✅ Gradient Checkpointing
#    STATUS: Verified in code, saves ~30% memory
#    TRADEOFF: 20-30% slower but allows larger batch sizes
#    CONFIG: training.use_gradient_checkpointing: true
#
# ✅ RoPE (Rotary Position Embeddings)
#    STATUS: Fully implemented, accepted by create_encoder()
#    BENEFIT: Better position encoding, resolution generalization
#    CONFIG: model.rope.use_rope: true (OPTIONAL - disable if unsure)
#
# ✅ FPN (Feature Pyramid Networks)
#    STATUS: Implemented and integrated
#    BENEFIT: Multi-scale feature learning
#    CONFIG: Disabled for safety (can enable if confident)
#
# ✅ Basic H-JEPA
#    STATUS: Core training loop proven to work
#    CONFIG: Standard hierarchical predictive architecture
#
# =============================================================================

experiment:
  name: "overnight_safe_baseline"
  seed: 42
  output_dir: "results/overnight_safe"
  save_frequency: 10
  eval_frequency: 10
  description: "Safe overnight training - verified features only, no experimental optimizations"

# =============================================================================
# MODEL CONFIGURATION - Conservative Baseline
# =============================================================================
model:
  # ViT-Small: 22M params, good balance of speed and capacity
  encoder_type: "vit_small_patch16_224"
  embed_dim: 384
  num_hierarchies: 3  # Standard 3-level hierarchy

  # DISABLED: Flash Attention (causes TypeError)
  # The HJEPA class passes use_flash_attention to create_encoder(),
  # but create_encoder() doesn't accept this parameter
  use_flash_attention: false

  # DISABLED: LayerScale (causes TypeError)
  # Same issue as Flash Attention - parameter mismatch
  # DO NOT include use_layerscale or layerscale section

  # NOTE: Gradient checkpointing is configured in training section below
  # (not here at model level)

  # OPTIONAL: RoPE (Rotary Position Embeddings)
  # This IS implemented and accepted by create_encoder()
  # If you want to be extra safe, set use_rope: false
  # If you want slightly better performance, set use_rope: true
  rope:
    use_rope: false  # Conservative: disabled for maximum safety
    theta: 10000.0   # Standard value if enabled

  # DISABLED: FPN (Feature Pyramid Networks)
  # While FPN is implemented, disable for conservative run
  fpn:
    use_fpn: false
    feature_dim: null
    fusion_method: "add"

  # Predictor configuration
  predictor:
    depth: 6
    num_heads: 6
    mlp_ratio: 4.0
    qkv_bias: true
    dropout: 0.0

  # Target encoder (EMA)
  ema:
    momentum: 0.996
    momentum_end: 1.0
    momentum_warmup_epochs: 10

# =============================================================================
# DATASET - Proven Working Combination
# =============================================================================
data:
  # Use multi-dataset with CIFAR-10 + STL-10
  # These datasets are small, fast, and proven to work
  use_multi_dataset: true
  data_path: "./data"  # Base path for datasets

  datasets:
    - name: cifar10
      weight: 0.5  # 50% CIFAR-10
      path: "./data"

    - name: stl10
      weight: 0.5  # 50% STL-10
      path: "./data"

  sampling_strategy: "weighted"
  image_size: 224

  # Batch size: Reduced for M1 Max memory constraints
  # 16 to prevent memory exhaustion (was 32, caused 76GB usage)
  batch_size: 16

  num_workers: 4  # Conservative worker count
  pin_memory: false  # Not needed for MPS

  # Standard JEPA augmentation (NOT DeiT III)
  # DeiT III augmentation is implemented but not integrated
  transforms:
    crop_scale: [0.8, 1.0]
    horizontal_flip: true
    color_jitter: 0.1

# =============================================================================
# TRAINING HYPERPARAMETERS - Conservative and Stable
# =============================================================================
training:
  # 45 epochs: Balance between time and convergence
  # Expected: ~10-11 min/epoch with gradient checkpointing
  # Total time: 45 × 10.5 = 7.5 hours (fits overnight window)
  epochs: 45

  warmup_epochs: 5

  # Learning rate: Conservative baseline
  optimizer: "adamw"
  lr: 0.00015  # Slightly higher than minimum safe value
  weight_decay: 0.04
  betas: [0.9, 0.95]

  # Learning rate schedule
  lr_schedule: "cosine"
  min_lr_ratio: 0.01
  warmup_lr_ratio: 0.001

  # Gradient clipping for stability
  clip_grad: 3.0

  # Mixed precision training (proven to work on MPS)
  use_amp: true

  # No gradient accumulation needed with batch_size=32
  accumulation_steps: 1

  # VERIFIED: Gradient checkpointing enabled
  use_gradient_checkpointing: true

# =============================================================================
# MASKING STRATEGY - Standard H-JEPA
# =============================================================================
masking:
  num_masks: 4
  mask_scale: [0.15, 0.2]
  aspect_ratio: [0.75, 1.5]
  num_context_masks: 1
  context_scale: [0.85, 1.0]

# =============================================================================
# LOSS CONFIGURATION - Combined JEPA + VICReg (COLLAPSE PREVENTION)
# =============================================================================
loss:
  # CRITICAL: Use "combined" to enable VICReg regularization
  # The previous "smoothl1" type created HJEPALoss which IGNORED all VICReg settings!
  # This was the root cause of representation collapse (all embeddings became identical)
  type: "combined"

  # Hierarchical weights for JEPA prediction loss
  hierarchy_weights: [1.0, 0.7, 0.5]

  normalize_embeddings: false

  # VICReg regularization (CRITICAL for collapse prevention)
  # Applied to context_features (encoder output) to maintain feature diversity
  # These flat keys match the factory's expected parameter names
  vicreg_weight: 0.1             # Global VICReg weight relative to JEPA loss
  vicreg_invariance_weight: 25.0 # Similarity between views
  vicreg_variance_weight: 25.0   # Variance term (prevents collapse!)
  vicreg_covariance_weight: 1.0  # Decorrelates features

  # NO contrastive component (C-JEPA not integrated)
  use_contrastive: false

# =============================================================================
# LOGGING
# =============================================================================
logging:
  use_wandb: false
  use_tensorboard: true
  log_frequency: 50

  log_images: true
  log_attention: false  # Disable for speed
  log_gradients: false
  log_dataset_distribution: true

# =============================================================================
# DEVICE
# =============================================================================
device: "mps"  # M1 Max acceleration

# =============================================================================
# CHECKPOINTING
# =============================================================================
checkpoint:
  save_best: true
  metric: "val_loss"
  mode: "min"
  keep_last_k: 3

# =============================================================================
# EVALUATION
# =============================================================================
evaluation:
  eval_frequency: 10

  # Quick k-NN evaluation during training
  knn:
    enabled: true
    k_values: [1, 5, 20]
    frequency: 10

# =============================================================================
# EXPECTED OUTCOMES
# =============================================================================
#
# TRAINING TIME:
#   - Per epoch: ~10-11 minutes (with gradient checkpointing)
#   - Total: 45 epochs × 10.5 min = 7.5-8 hours
#
# PERFORMANCE TARGETS:
#   - Linear probe accuracy: 55-65%
#   - k-NN accuracy (k=20): 50-60%
#   - Final loss: < 0.6
#   - No representation collapse
#
# MEMORY USAGE:
#   - Peak: 10-14 GB RAM (well within 32GB limit)
#   - GPU (MPS): 7-9 GB
#
# VALIDATION CHECKPOINTS:
#   - Epoch 10: Loss should be decreasing steadily
#   - Epoch 25: k-NN should show 35-45% accuracy
#   - Epoch 35: Near-final performance visible
#   - Epoch 45: Final evaluation
#
# SUCCESS CRITERIA:
#   ✓ Training completes in < 8.5 hours
#   ✓ No crashes or TypeErrors
#   ✓ No NaN/Inf losses
#   ✓ Linear probe > 55%
#   ✓ Feature variance > 0.1 (no collapse)
#   ✓ Smooth loss curves
#
# =============================================================================
# TROUBLESHOOTING GUIDE
# =============================================================================
#
# If training still fails:
#
# 1. Out of Memory:
#    - Reduce batch_size: 32 → 24 → 16
#    - Reduce num_workers: 4 → 2
#    - Reduce num_hierarchies: 3 → 2
#
# 2. Training instability (NaN/divergence):
#    - Reduce learning rate: 0.00015 → 0.0001
#    - Increase warmup: 5 → 10 epochs
#    - Increase gradient clipping: 3.0 → 1.0
#
# 3. Too slow (> 12 min/epoch):
#    - Reduce epochs: 45 → 35 → 30
#    - Reduce num_workers: 4 → 2
#    - Disable log_images: false
#    - Disable gradient_checkpointing (if memory allows)
#
# 4. Dataset loading errors:
#    - Verify data exists: ls ./data/cifar-10-batches-py
#    - Verify data exists: ls ./data/stl10_binary
#    - Try single dataset instead of multi-dataset
#
# 5. MPS backend issues:
#    - Fall back to CPU: device: "cpu"
#    - Reduce batch_size: 32 → 16
#    - Disable use_amp: false
#
# =============================================================================
# HOW TO RUN
# =============================================================================
#
# 1. Verify data is downloaded:
#    python -c "from torchvision import datasets; datasets.CIFAR10('./data', download=True); datasets.STL10('./data', download=True)"
#
# 2. Start training:
#    python scripts/train.py --config configs/overnight_safe.yaml
#
# 3. Monitor progress:
#    python monitor_training.py results/overnight_safe
#
# 4. If it crashes, check the logs:
#    tail -f results/overnight_safe/logs/train.log
#
# =============================================================================
# NEXT STEPS AFTER SUCCESSFUL RUN
# =============================================================================
#
# Once this configuration completes successfully:
#
# 1. Document baseline performance
#    - Save linear probe accuracy
#    - Save final loss value
#    - Save training time per epoch
#
# 2. Enable RoPE for slight improvement:
#    - Set model.rope.use_rope: true
#    - Should add +1-2% accuracy with minimal cost
#
# 3. Try larger model if performance is good:
#    - vit_small → vit_base
#    - Adjust batch_size if needed
#
# 4. Report issues with Flash Attention and LayerScale:
#    - These need code fixes to work properly
#    - create_encoder() signature needs updating
#
# =============================================================================
